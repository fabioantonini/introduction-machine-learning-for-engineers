{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0721c52-89a2-4c53-a650-e18499471ade",
   "metadata": {},
   "source": [
    "### Feature Scaling\r\n",
    "\r\n",
    "#### What is Feature Scaling?\r\n",
    "\r\n",
    "Feature scaling is a technique used to normalize the range of independent variables or features in a dataset. It is also known as data normalization and is a crucial step in the preprocessing stage of machine learning models. \r\n",
    "\r\n",
    "#### Issues Addressed by Feature Scaling\r\n",
    "\r\n",
    "1. **Different Ranges**: Features in a dataset might have different units and scales (e.g., age might range from 0 to 100, while income might range from 0 to 100,000). This disparity can lead to biased results, particularly in algorithms that rely on distance calculations like k-nearest neighbors (KNN) and support vector machines (SVM).\r\n",
    "2. **Gradient Descent Convergence**: Feature scaling helps in faster convergence of the gradient descent algorithm used in optimization problems. When features are on different scales, the gradients can oscillate inefficiently, slowing down the convergence.\r\n",
    "3. **Interpretability**: Scaling features to a common range makes it easier to interpret the coefficients of linear models.\r\n",
    "\r\n",
    "#### Most Used Approaches for Feature Scaling\r\n",
    "\r\n",
    "1. **Min-Max Scaling (Normalization)**:\r\n",
    "   - Rescales the feature to a fixed range, usually 0 to 1.\r\n",
    "   - Formula: \r\n",
    "     $$ X' = \\frac{X - X_{min}}{X_{max} - X_{min}} $$\r\n",
    "   - Suitable for algorithms that do not assume any distribution of the data.\r\n",
    "\r\n",
    "2. **Standardization (Z-score Normalization)**:\r\n",
    "   - Rescales the data to have a mean of 0 and a standard deviation of 1.\r\n",
    "   - Formula: \r\n",
    "     $$ X' = \\frac{X - \\mu}{\\sigma} $$\r\n",
    "   - Suitable for algorithms that assume a Gaussian distribution in the data.\r\n",
    "\r\n",
    "3. **Robust Scaling**:\r\n",
    "   - Uses the median and the interquartile range to scale features.\r\n",
    "   - Formula:\r\n",
    "     $$ X' = \\frac{X - \\text{median}}{IQR} $$\r\n",
    "   - Suitable for datasets with outliers.\r\n",
    "\r\n",
    "4. **MaxAbs Scaling**:\r\n",
    "   - Scales each feature by its maximum absolute value.\r\n",
    "   - Formula:\r\n",
    "     $$ X' = \\frac{X}{\\text{max}(|Xte equally to the analysis, thereby improving the performance of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29297a04-d9b8-480b-b149-094a25565e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate sample data\n",
    "data = {\n",
    "    'Feature1': np.random.randint(1, 100, 100),\n",
    "    'Feature2': np.random.rand(100) * 1000,\n",
    "    'Feature3': np.random.randn(100) * 100\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize scalers\n",
    "min_max_scaler = MinMaxScaler()\n",
    "standard_scaler = StandardScaler()\n",
    "max_abs_scaler = MaxAbsScaler()\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# Apply scaling\n",
    "df_min_max_scaled = min_max_scaler.fit_transform(df)\n",
    "df_standard_scaled = standard_scaler.fit_transform(df)\n",
    "df_max_abs_scaled = max_abs_scaler.fit_transform(df)\n",
    "df_robust_scaled = robust_scaler.fit_transform(df)\n",
    "\n",
    "# Convert to DataFrame for easier plotting\n",
    "df_min_max_scaled = pd.DataFrame(df_min_max_scaled, columns=df.columns)\n",
    "df_standard_scaled = pd.DataFrame(df_standard_scaled, columns=df.columns)\n",
    "df_max_abs_scaled = pd.DataFrame(df_max_abs_scaled, columns=df.columns)\n",
    "df_robust_scaled = pd.DataFrame(df_robust_scaled, columns=df.columns)\n",
    "\n",
    "# Plot original and scaled data\n",
    "fig, axs = plt.subplots(5, 1, figsize=(10, 20))\n",
    "axs[0].set_title('Original Data')\n",
    "df.plot(ax=axs[0])\n",
    "axs[1].set_title('Min-Max Scaled Data')\n",
    "df_min_max_scaled.plot(ax=axs[1])\n",
    "axs[2].set_title('Standard Scaled Data')\n",
    "df_standard_scaled.plot(ax=axs[2])\n",
    "axs[3].set_title('Max Abs Scaled Data')\n",
    "df_max_abs_scaled.plot(ax=axs[3])\n",
    "axs[4].set_title('Robust Scaled Data')\n",
    "df_robust_scaled.plot(ax=axs[4])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc47d33-0d44-4d3f-ab3a-382b2db6f011",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "1. **Original Data**: The initial dataset with three features having different scales.\n",
    "2. **Min-Max Scaling**: Rescales the features to a range of [0, 1].\n",
    "3. **Standardization**: Rescales the data to have a mean of 0 and standard deviation of 1.\n",
    "4. **Robust Scaling**: Uses median and IQR to scale the data, making it robust to outliers.\n",
    "5. **MaxAbs Scaling**: Scales each feature by its maximum absolute value.\n",
    "\n",
    "These methods ensure that features contribute equally to the analysis, thereby improving the performance of machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
