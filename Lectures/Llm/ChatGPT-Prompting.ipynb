{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1019051-3230-4c11-8ceb-a5cbe5951866",
   "metadata": {},
   "source": [
    "# Guidelines for Prompting\n",
    "In this lesson, you'll practice two prompting principles and their related tactics in order to write effective prompts for large language models.\n",
    "\n",
    "## Setup\n",
    "#### Load the API key and relevant Python libaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe69cc90-56fe-499f-b4ba-73cff24fc5eb",
   "metadata": {},
   "source": [
    "In this course, we've provided some code that loads the OpenAI API key for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9df9bd46-8c66-4b68-932e-4293508e66af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25ec531-74da-490c-807c-400123454a31",
   "metadata": {},
   "source": [
    "#### helper function\n",
    "Throughout this course, we will use OpenAI's `gpt-3.5-turbo` model and the [chat completions endpoint](https://platform.openai.com/docs/guides/chat). \n",
    "\n",
    "This helper function will make it easier to use prompts and look at the generated outputs.  \n",
    "**Note**: In June 2023, OpenAI updated gpt-3.5-turbo. The results you see in the notebook may be slightly different than those in the video. Some of the prompts have also been slightly modified to product the desired results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d83eb527-842c-434a-89ae-037286e34e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89140013-9b43-48ab-9365-e91e43526e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-4o\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65013476-3ae4-4bc8-ab85-e7bcf9e57277",
   "metadata": {},
   "source": [
    "## Prompting Principles\n",
    "- **Principle 1: Write clear and specific instructions**\n",
    "- **Principle 2: Give the model time to “think”**\n",
    "\n",
    "### Tactics\n",
    "\n",
    "#### Tactic 1: Use delimiters to clearly indicate distinct parts of the input\n",
    "- Delimiters can be anything like: ```, \"\"\", < >, `<tag> </tag>`, `:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8223d0e0-eef7-4e76-a98d-aa89e77cc6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"\"\"\n",
    "Let's look at some\n",
    "concrete numbers for what J-train and JCV might be, and see how you can judge if a learning algorithm has\n",
    "high bias or high variance. For the examples in this video, I'm going to use as a running\n",
    "example the application of speech recognition which is something I've worked on\n",
    "multiple times over the years. Let's take a look. A\n",
    "lot of users doing web search on a\n",
    "mobile phone will use speech recognition\n",
    "rather than type on the tiny keyboards on\n",
    "our phones because speaking to a phone is\n",
    "often faster than typing. Typical audio that's\n",
    "a web search engine we get would be like this, \"What is today's weather?\" Or like this, \"Coffee shops near me.\" It's the job of the speech recognition algorithms to output the transcripts whether it's today's weather or\n",
    "coffee shops near me. Now, if you were to train a speech recognition system and measure the training error, and the training\n",
    "error means what's the percentage of audio clips in your training set that\n",
    "the algorithm does not transcribe correctly\n",
    "in its entirety. Let's say the training error for this data-set is\n",
    "10.8 percent meaning that it transcribes it perfectly for 89.2 percent of\n",
    "your training set, but makes some mistake in 10.8 percent of\n",
    "your training set. If you were to also measure your speech recognition\n",
    "algorithm's performance on a separate\n",
    "cross-validation set, let's say it gets\n",
    "14.8 percent error. If you were to look at these numbers it looks like the training error\n",
    "is really high, it got 10 percent wrong, and then the cross-validation\n",
    "error is higher but getting 10 percent of even\n",
    "your training set wrong that seems pretty high. It seems like that 10 percent\n",
    "error would lead you to conclude it has high bias because it's not doing\n",
    "well on your training set, but it turns out\n",
    "that when analyzing speech recognition\n",
    "it's useful to also measure one other\n",
    "thing which is what is the human level of performance? In other words, how\n",
    "well can even humans transcribe speech accurately\n",
    "from these audio clips? Concretely, let's\n",
    "say that you measure how well fluent speakers can transcribe audio\n",
    "clips and you find that it is 10.6 percent, and you find that human\n",
    "level performance achieves 10.6 percent error. Why is human level\n",
    "error so high? It turns out that\n",
    "for web search, there are a lot of audio\n",
    "clips that sound like this, \"I'm going to navigate\n",
    "to [inaudible].\" There's a lot of\n",
    "noisy audio where really no one can accurately transcribe what was said because of the\n",
    "noise in the audio. If even a human makes\n",
    "10.6 percent error, then it seems difficult to expect a learning algorithm\n",
    "to do much better. In order to judge if the\n",
    "training error is high, it turns out to be\n",
    "more useful to see if the training error is much higher than a human\n",
    "level of performance, and in this example it does just 0.2 percent\n",
    "worse than humans. Given that humans are\n",
    "actually really good at recognizing speech I think if I can build a speech recognition\n",
    "system that achieves 10.6 percent error matching human performance\n",
    "I'd be pretty happy, so it's just doing a little\n",
    "bit worse than humans. But in contrast, the\n",
    "gap or the difference between JCV and J-train\n",
    "is much larger. There's actually a four\n",
    "percent gap there, whereas previously\n",
    "we had said maybe 10.8 percent error means\n",
    "this is high bias. When we benchmark it to\n",
    "human level performance, we see that the\n",
    "algorithm is actually doing quite well on\n",
    "the training set, but the bigger problem is the cross-validation\n",
    "error is much higher than the\n",
    "training error which is why I would conclude that this algorithm actually has more of a variance problem\n",
    "than a bias problem. It turns out when judging if the training error is high is often useful to establish a baseline level of performance, and by baseline level\n",
    "of performance I mean what is the level of error you can\n",
    "reasonably hope your learning algorithm to\n",
    "eventually get to. One common way to establish a baseline level of performance\n",
    "is to measure how well humans can do on\n",
    "this task because humans are really good at\n",
    "understanding speech data, or processing images or\n",
    "understanding texts. Human level performance is often a good benchmark when you\n",
    "are using unstructured data, such as: audio, images, or texts. Another way to estimate a baseline level\n",
    "of performance is if there's some\n",
    "competing algorithm, maybe a previous implementation that someone else has\n",
    "implemented or even a competitor's\n",
    "algorithm to establish a baseline level of performance\n",
    "if you can measure that, or sometimes you might guess\n",
    "based on prior experience. If you have access to this baseline level of\n",
    "performance that is, what is the level of error you can reasonably\n",
    "hope to get to or what is the desired level of performance that you want\n",
    "your algorithm to get to? Then when judging if an algorithm has high\n",
    "bias or variance, you would look at the baseline\n",
    "level of performance, and the training error, and the cross-validation error. The two key quantities\n",
    "to measure are then: what is the difference between training error and the baseline level that\n",
    "you hope to get to. This is 0.2, and if this is large then you would say you have\n",
    "a high bias problem. You will then also look at this gap between\n",
    "your training error and your cross-validation error, and if this is high then you will conclude you have a\n",
    "high variance problem. That's why in this example we concluded we have a\n",
    "high variance problem, whereas let's look at\n",
    "the second example. If the baseline level\n",
    "of performance; that is human level performance,\n",
    "and training error, and cross validation\n",
    "error look like this, then this first gap is 4.4 percent and so\n",
    "there's actually a big gap. The training error\n",
    "is much higher than what humans can\n",
    "do and what we hope to get to whereas the\n",
    "cross-validation error is just a little bit bigger\n",
    "than the training error. If your training error and cross validation\n",
    "error look like this, I will say this\n",
    "algorithm has high bias. By looking at these numbers, training error and\n",
    "cross validation error, you can get a sense\n",
    "intuitively or informally of the degree to which\n",
    "your algorithm has a high bias or\n",
    "high variance problem. Just to summarize,\n",
    "this gap between these first two\n",
    "numbers gives you a sense of whether you\n",
    "have a high bias problem, and the gap between these\n",
    "two numbers gives you a sense of whether you have\n",
    "a high variance problem. Sometimes the baseline level of performance could\n",
    "be zero percent. If your goal is to achieve perfect performance\n",
    "than the baseline level of performance it\n",
    "could be zero percent, but for some applications like the speech recognition\n",
    "application where some audio is just noisy then\n",
    "the baseline level of a performance could be\n",
    "much higher than zero. The method described\n",
    "on this slide will give you a better read in terms of whether your algorithm suffers from bias or variance. By the way, it is possible for your algorithms to have high\n",
    "bias and high variance. Concretely, if you get\n",
    "numbers like these, then the gap between the baseline and the\n",
    "training error is large. That would be a 4.6 percent, and the gap between training error and cross\n",
    "validation error is also large. This is 4.2 percent. If it looks like this\n",
    "you will conclude that your algorithm has high\n",
    "bias and high variance, although hopefully\n",
    "this won't happen that often for your\n",
    "learning applications. To summarize, we've seen that looking at whether\n",
    "your training error is large is a way to tell if\n",
    "your algorithm has high bias, but on applications\n",
    "where the data is sometimes just noisy\n",
    "and is infeasible or unrealistic to\n",
    "ever expect to get a zero error then it's useful to establish this baseline\n",
    "level of performance. Rather than just asking is\n",
    "my training error a lot, you can ask is my training error large relative to what I hope\n",
    "I can get to eventually, such as, is my training large relative to what\n",
    "humans can do on the task? That gives you a more\n",
    "accurate read on how far away you are in terms of your training error from\n",
    "where you hope to get to. Then similarly, looking at whether your\n",
    "cross-validation error is much larger than\n",
    "your training error, gives you a sense\n",
    "of whether or not your algorithm may have a high\n",
    "variance problem as well. In practice, this is how I often will look at these\n",
    "numbers to judge if my learning algorithm has a high bias or high\n",
    "variance problem. Now, to further hone our intuition about how a\n",
    "learning algorithm is doing, there's one other\n",
    "thing that I found useful to think about which\n",
    "is the learning curve. Let's take a look at what\n",
    "that means in the next video.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9e6a481e-8dbd-45f8-a447-73cd52473a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_no_cr = f\"\"\"\n",
    "Remove any Carriage returns from the text delimited by triple backticks. Replace the word 'video' with 'lecture'.\n",
    "```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "27ad88f1-b640-4158-b652-aed20487f198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's look at some concrete numbers for what J-train and JCV might be, and see how you can judge if a learning algorithm has high bias or high variance. For the examples in this lecture, I'm going to use as a running example the application of speech recognition which is something I've worked on multiple times over the years. Let's take a look. A lot of users doing web search on a mobile phone will use speech recognition rather than type on the tiny keyboards on our phones because speaking to a phone is often faster than typing. Typical audio that's a web search engine we get would be like this, \"What is today's weather?\" Or like this, \"Coffee shops near me.\" It's the job of the speech recognition algorithms to output the transcripts whether it's today's weather or coffee shops near me. Now, if you were to train a speech recognition system and measure the training error, and the training error means what's the percentage of audio clips in your training set that the algorithm does not transcribe correctly in its entirety. Let's say the training error for this data-set is 10.8 percent meaning that it transcribes it perfectly for 89.2 percent of your training set, but makes some mistake in 10.8 percent of your training set. If you were to also measure your speech recognition algorithm's performance on a separate cross-validation set, let's say it gets 14.8 percent error. If you were to look at these numbers it looks like the training error is really high, it got 10 percent wrong, and then the cross-validation error is higher but getting 10 percent of even your training set wrong that seems pretty high. It seems like that 10 percent error would lead you to conclude it has high bias because it's not doing well on your training set, but it turns out that when analyzing speech recognition it's useful to also measure one other thing which is what is the human level of performance? In other words, how well can even humans transcribe speech accurately from these audio clips? Concretely, let's say that you measure how well fluent speakers can transcribe audio clips and you find that it is 10.6 percent, and you find that human level performance achieves 10.6 percent error. Why is human level error so high? It turns out that for web search, there are a lot of audio clips that sound like this, \"I'm going to navigate to [inaudible].\" There's a lot of noisy audio where really no one can accurately transcribe what was said because of the noise in the audio. If even a human makes 10.6 percent error, then it seems difficult to expect a learning algorithm to do much better. In order to judge if the training error is high, it turns out to be more useful to see if the training error is much higher than a human level of performance, and in this example it does just 0.2 percent worse than humans. Given that humans are actually really good at recognizing speech I think if I can build a speech recognition system that achieves 10.6 percent error matching human performance I'd be pretty happy, so it's just doing a little bit worse than humans. But in contrast, the gap or the difference between JCV and J-train is much larger. There's actually a four percent gap there, whereas previously we had said maybe 10.8 percent error means this is high bias. When we benchmark it to human level performance, we see that the algorithm is actually doing quite well on the training set, but the bigger problem is the cross-validation error is much higher than the training error which is why I would conclude that this algorithm actually has more of a variance problem than a bias problem. It turns out when judging if the training error is high is often useful to establish a baseline level of performance, and by baseline level of performance I mean what is the level of error you can reasonably hope your learning algorithm to eventually get to. One common way to establish a baseline level of performance is to measure how well humans can do on this task because humans are really good at understanding speech data, or processing images or understanding texts. Human level performance is often a good benchmark when you are using unstructured data, such as: audio, images, or texts. Another way to estimate a baseline level of performance is if there's some competing algorithm, maybe a previous implementation that someone else has implemented or even a competitor's algorithm to establish a baseline level of performance if you can measure that, or sometimes you might guess based on prior experience. If you have access to this baseline level of performance that is, what is the level of error you can reasonably hope to get to or what is the desired level of performance that you want your algorithm to get to? Then when judging if an algorithm has high bias or variance, you would look at the baseline level of performance, and the training error, and the cross-validation error. The two key quantities to measure are then: what is the difference between training error and the baseline level that you hope to get to. This is 0.2, and if this is large then you would say you have a high bias problem. You will then also look at this gap between your training error and your cross-validation error, and if this is high then you will conclude you have a high variance problem. That's why in this example we concluded we have a high variance problem, whereas let's look at the second example. If the baseline level of performance; that is human level performance, and training error, and cross validation error look like this, then this first gap is 4.4 percent and so there's actually a big gap. The training error is much higher than what humans can do and what we hope to get to whereas the cross-validation error is just a little bit bigger than the training error. If your training error and cross validation error look like this, I will say this algorithm has high bias. By looking at these numbers, training error and cross validation error, you can get a sense intuitively or informally of the degree to which your algorithm has a high bias or high variance problem. Just to summarize, this gap between these first two numbers gives you a sense of whether you have a high bias problem, and the gap between these two numbers gives you a sense of whether you have a high variance problem. Sometimes the baseline level of performance could be zero percent. If your goal is to achieve perfect performance than the baseline level of performance it could be zero percent, but for some applications like the speech recognition application where some audio is just noisy then the baseline level of a performance could be much higher than zero. The method described on this slide will give you a better read in terms of whether your algorithm suffers from bias or variance. By the way, it is possible for your algorithms to have high bias and high variance. Concretely, if you get numbers like these, then the gap between the baseline and the training error is large. That would be a 4.6 percent, and the gap between training error and cross validation error is also large. This is 4.2 percent. If it looks like this you will conclude that your algorithm has high bias and high variance, although hopefully this won't happen that often for your learning applications. To summarize, we've seen that looking at whether your training error is large is a way to tell if your algorithm has high bias, but on applications where the data is sometimes just noisy and is infeasible or unrealistic to ever expect to get a zero error then it's useful to establish this baseline level of performance. Rather than just asking is my training error a lot, you can ask is my training error large relative to what I hope I can get to eventually, such as, is my training large relative to what humans can do on the task? That gives you a more accurate read on how far away you are in terms of your training error from where you hope to get to. Then similarly, looking at whether your cross-validation error is much larger than your training error, gives you a sense of whether or not your algorithm may have a high variance problem as well. In practice, this is how I often will look at these numbers to judge if my learning algorithm has a high bias or high variance problem. Now, to further hone our intuition about how a learning algorithm is doing, there's one other thing that I found useful to think about which is the learning curve. Let's take a look at what that means in the next lecture.\n"
     ]
    }
   ],
   "source": [
    "response_no_cr = get_completion(prompt_no_cr)\n",
    "print(response_no_cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6b8dcef-31d7-4fdd-af54-878f670b26d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_summary = f\"\"\"\n",
    "Then summarize the 'response' in no more than 6 numbered lines with carriage return.\n",
    "```{response_no_cr}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "20220eee-cf54-4e92-8b09-0960fd4523d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **Training and Cross-Validation Errors**: Training error of 10.8% and cross-validation error of 14.8% suggest high bias and variance.\n",
      "2. **Human-Level Performance**: Human error rate is 10.6%, indicating the algorithm's training error is close to human performance.\n",
      "3. **Bias vs. Variance**: A small gap between training error and human-level performance indicates low bias, while a large gap between training and cross-validation errors indicates high variance.\n",
      "4. **Baseline Performance**: Establishing a baseline (e.g., human-level performance) helps judge if training error is high.\n",
      "5. **Error Gaps**: Large gap between training error and baseline indicates high bias; large gap between training and cross-validation errors indicates high variance.\n",
      "6. **Combined Issues**: An algorithm can have both high bias and high variance if both gaps are large.\n"
     ]
    }
   ],
   "source": [
    "response_summary = get_completion(prompt_summary)\n",
    "print(response_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "47858772-5f44-4510-8b71-27b477c78859",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_image = f\"\"\"\n",
    "Please can help me to find a picture describing any of the topics listed in the previous summary?\n",
    "```{response_summary}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "666dad13-f715-4913-a4ef-b8fb258de86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! To find a picture that describes any of the topics listed in the summary, you can search for images related to \"Bias vs. Variance tradeoff in machine learning\" or \"Training and Cross-Validation Errors in machine learning.\" These images typically illustrate the concepts of high bias, high variance, and the relationship between training error, cross-validation error, and human-level performance.\n",
      "\n",
      "Here are some steps to help you find such an image:\n",
      "\n",
      "1. **Search Keywords**: Use search engines or image repositories with keywords like:\n",
      "   - \"Bias vs. Variance tradeoff\"\n",
      "   - \"Training and Cross-Validation Errors\"\n",
      "   - \"Machine Learning Error Analysis\"\n",
      "   - \"High Bias and High Variance\"\n",
      "\n",
      "2. **Image Repositories**: Check popular image repositories and educational websites:\n",
      "   - Google Images\n",
      "   - Wikimedia Commons\n",
      "   - Educational websites like Coursera, edX, or Khan Academy\n",
      "\n",
      "3. **Example Search**:\n",
      "   - Go to Google Images.\n",
      "   - Enter the search term \"Bias vs. Variance tradeoff in machine learning.\"\n",
      "   - Look for diagrams that show training error, cross-validation error, and human-level performance.\n",
      "\n",
      "4. **Specific Diagrams**: Look for diagrams that include:\n",
      "   - A plot showing training error and cross-validation error over different model complexities.\n",
      "   - A visual representation of high bias (underfitting) and high variance (overfitting).\n",
      "   - Comparisons between training error, cross-validation error, and human-level performance.\n",
      "\n",
      "Here is an example of what you might find:\n",
      "\n",
      "![Bias vs. Variance Tradeoff](https://miro.medium.com/max/1400/1*8q5F5Xb5J5Xb5J5Xb5J5Xb5.png)\n",
      "\n",
      "This image illustrates the concept of bias and variance, showing how training error and cross-validation error change with model complexity. It helps to understand the tradeoff between underfitting (high bias) and overfitting (high variance).\n",
      "\n",
      "If you need a specific type of image or further assistance, please let me know!\n"
     ]
    }
   ],
   "source": [
    "response_summary = get_completion(prompt_image)\n",
    "print(response_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fdd64c-1445-42b2-96dc-f477f926a2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
